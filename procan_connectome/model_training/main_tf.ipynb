{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "from procan_connectome.model_training.loocv_wrapper import LOOCV_Wrapper\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "# from procan_connectome.data_processing.linear_svc_importance_filter import LinearSVCImportanceFilter\n",
    "# from procan_connectome.data_processing.correlation_filter import CorrelationFilter\n",
    "# from procan_connectome.data_processing.powertransformer_wrapper import PowerTransformerWrapper\n",
    "# from procan_connectome.data_processing.select_k_best_filter import SelectKBestFilter\n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn.decomposition import FastICA\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import datetime\n",
    "from procan_connectome.config import DATA_PATH, RANDOM_STATE, LOGGER_LEVEL\n",
    "import tensorflow as tf "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, 'combined_datasets.csv'))\n",
    "df = df.set_index('ID')\n",
    "X, y = df.drop(columns=['label']), df['label']\n",
    "log_dir = os.path.join(DATA_PATH, 'logs')\n",
    "pipeline =  Pipeline([\n",
    "    ('ss', StandardScaler()),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "ohe = OneHotEncoder(sparse=False)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X,y, test_size=0.2)\n",
    "y_dev = ohe.fit_transform(np.array(y_dev).reshape(-1, 1))\n",
    "y_test = ohe.transform(np.array(y_test).reshape(-1, 1))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.5)\n",
    "ss = StandardScaler() \n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_val = ss.transform(X_val)\n",
    "X_test = ss.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "def get_fcnn_model(input_shape=(1359, ), k=5, lr=1e-5): \n",
    "    model_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Dense(1359, activation='relu')(model_input)\n",
    "    x = tf.keras.layers.Dense(680, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(340, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(170, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(85, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(42, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(21, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "    model_output = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs= model_input, outputs=model_output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model \n",
    "    \n",
    "model = get_fcnn_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 1359)]            0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1359)              1848240   \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 680)               924800    \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 340)               231540    \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 170)               57970     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 85)                14535     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 42)                3612      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 21)                903       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10)                220       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 3,081,875\n",
      "Trainable params: 3,081,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "model_name_fcn =  os.path.join(DATA_PATH, 'tf', datetime.datetime.now().strftime('%Y_%m_%d_%H_%M') + \"_tf.h5\")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "monitor = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_name_fcn,\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min')\n",
    "\n",
    "# Learning rate schedule\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch%10 == 0 and epoch != 0 :\n",
    "        lr = lr/2\n",
    "    return lr\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          steps_per_epoch=len(X_train) / 32,\n",
    "          epochs=100,\n",
    "          verbose=3,\n",
    "          callbacks = [early_stop, monitor, lr_schedule],\n",
    "          validation_data = (X_val, y_val))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63692, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.63692 to 1.63456, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.63456 to 1.63268, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.63268 to 1.63069, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.63069 to 1.62894, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.62894 to 1.62713, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.62713 to 1.62557, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.62557 to 1.62413, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.62413 to 1.62297, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 9.999999747378752e-06.\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.62297 to 1.62180, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.62180 to 1.62116, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.62116 to 1.62056, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.62056 to 1.62007, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.62007 to 1.61953, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.61953 to 1.61881, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.61881 to 1.61816, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.61816 to 1.61773, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.61773 to 1.61742, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.61742 to 1.61712, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 4.999999873689376e-06.\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.61712 to 1.61686, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.61686 to 1.61668, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.61668 to 1.61652, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.61652 to 1.61640, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.61640 to 1.61627, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.61627 to 1.61623, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.61623\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.61623 to 1.61623, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.61623 to 1.61619, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.61619 to 1.61613, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 2.499999936844688e-06.\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.61613 to 1.61607, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.61607 to 1.61604, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.61604 to 1.61601, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.61601 to 1.61598, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.61598 to 1.61594, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.61594 to 1.61586, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.61586 to 1.61578, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.61578 to 1.61570, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.61570 to 1.61565, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.61565 to 1.61565, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 1.249999968422344e-06.\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.61565 to 1.61564, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.61564 to 1.61564, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.61564 to 1.61563, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.61563 to 1.61562, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.61562\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.61562 to 1.61561, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.61561 to 1.61561, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00047: val_loss improved from 1.61561 to 1.61560, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00048: val_loss improved from 1.61560 to 1.61558, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00049: val_loss improved from 1.61558 to 1.61556, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 6.24999984211172e-07.\n",
      "\n",
      "Epoch 00050: val_loss improved from 1.61556 to 1.61555, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00051: val_loss improved from 1.61555 to 1.61554, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00052: val_loss improved from 1.61554 to 1.61553, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00053: val_loss improved from 1.61553 to 1.61551, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00054: val_loss improved from 1.61551 to 1.61550, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00055: val_loss improved from 1.61550 to 1.61550, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00056: val_loss improved from 1.61550 to 1.61549, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00057: val_loss improved from 1.61549 to 1.61546, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00058: val_loss improved from 1.61546 to 1.61544, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00059: val_loss improved from 1.61544 to 1.61543, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 3.12499992105586e-07.\n",
      "\n",
      "Epoch 00060: val_loss improved from 1.61543 to 1.61541, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00061: val_loss improved from 1.61541 to 1.61540, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00062: val_loss improved from 1.61540 to 1.61539, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00063: val_loss improved from 1.61539 to 1.61539, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00064: val_loss improved from 1.61539 to 1.61538, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.61538 to 1.61537, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00066: val_loss improved from 1.61537 to 1.61536, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00067: val_loss improved from 1.61536 to 1.61535, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00068: val_loss improved from 1.61535 to 1.61534, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.61534 to 1.61533, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 1.56249996052793e-07.\n",
      "\n",
      "Epoch 00070: val_loss improved from 1.61533 to 1.61532, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00071: val_loss improved from 1.61532 to 1.61532, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00072: val_loss improved from 1.61532 to 1.61532, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.61532\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.61532\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.61532\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.61532\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.61532\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.61532\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.61532\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 7.81249980263965e-08.\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.61532\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.61532\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.61532\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00083: val_loss improved from 1.61532 to 1.61532, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00084: val_loss improved from 1.61532 to 1.61532, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00085: val_loss improved from 1.61532 to 1.61532, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00086: val_loss improved from 1.61532 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00087: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00088: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00089: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 3.906249901319825e-08.\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00091: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00092: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00093: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00094: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00095: val_loss improved from 1.61531 to 1.61531, saving model to /home/mklasby/Sandbox/Research/procan_connectome/procan_connectome/data/tf/2021_07_20_15_06_tf.h5\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.61531\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.61531\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.61531\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.61531\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 1.9531249506599124e-08.\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.61531\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd7c80b9910>"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "metrics = model.evaluate(X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 1.6170 - accuracy: 0.2857\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "\n",
    "\n",
    "log_file_name = f\"{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')}\" + \"_\" + 'TF'\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(DATA_PATH, 'logs', log_file_name + \"_LOGS\"),\n",
    "    filemode='a',\n",
    "    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=LOGGER_LEVEL\n",
    ")\n",
    "\n",
    "loocv = LOOCV_Wrapper(\n",
    "            X, \n",
    "            y, \n",
    "            estimator, \n",
    "            pipeline=pipeline, \n",
    "            param_grid=grid,\n",
    "            perform_grid_search=False,\n",
    "            label_col='label',\n",
    "            log_file_name = log_file_name,\n",
    "            log_dir=log_dir,\n",
    "            balance_classes=True, \n",
    "            scoring='f1_weighted',\n",
    "            verbose=2,\n",
    "            n_samples=None,\n",
    "            single_label_upsample=None,\n",
    "            cv=None,\n",
    "            select_features=False,\n",
    "            feature_threshold=0.001,\n",
    "            grid_search_feature_selection=False\n",
    "        )\n",
    "        loocv.fit(X,y) "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-78-416f82b5dd6c>, line 31)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-78-416f82b5dd6c>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    loocv.fit(X,y)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}